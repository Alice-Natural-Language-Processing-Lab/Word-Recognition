from keras.callbacks import ModelCheckpoint
from keras.callbacks import EarlyStopping
from keras.preprocessing.image import ImageDataGenerator
from generate_data import generate_10k, generate_val


def train(model, mode):#, X_train, X_test, Words_train, Words_test, y_train, y_test):
    batch_size = 32
    nb_epoch = 1
    checkpointer = ModelCheckpoint( filepath="weights.hdf5", verbose=1, monitor='val_loss',
                                    save_best_only=False )

    early_stopping = EarlyStopping( monitor='val_loss', patience=10, verbose=0, mode='auto' )

    if mode == 0:  # simple training without data loading in chunks
        print('Not using data augmentation.')
        # past_history = model.fit( [X_train, Words_train], y_train,
        #                           batch_size=batch_size,
        #                           nb_epoch=nb_epoch,
        #                           shuffle=True,
        #                           callbacks=[checkpointer, early_stopping] )
    elif mode == 1:  # data augmentation mode
        print('Using real-time data augmentation.')

        # this will do pre-processing and realtime data augmentation
        # datagen = ImageDataGenerator( rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)
        #                               height_shift_range=0.10,
        #                               # randomly shift images horizontally (fraction of total width)
        #                               width_shift_range=0.10,
        #                               # randomly shift images vertically (fraction of total height)
        #                               vertical_flip=False,
        #                               zoom_range=0.1,
        #                               shear_range=0.1,
        #                               zca_whitening=False,
        #                               fill_mode="nearest" )  # randomly flip images
        #
        # # compute quantities required for featurewise normalization
        # # (std, mean, and principal components if ZCA whitening is applied)
        # datagen.fit( X_train )
        # # fit the model on the batches generated by datagen.flow()
        # past_history = model.fit_generator( datagen.flow( [X_train, Words_train], y_train,
        #                                                   batch_size=batch_size ),
        #                                     samples_per_epoch=X_train.shape[0],
        #                                     nb_epoch=nb_epoch,
        #                                     validation_data=([X_test, Words_test], y_test),
        #                                     callbacks=[checkpointer, early_stopping] )
    else:  # data loading in chunks
        batch_size = 16
        train_gen = generate_10k(batch_size=batch_size)
        val_gen = generate_val(batch_size=32)
        past_history = []
        print ('data loading in chunks from hdf5 format')
        past_history = model.fit_generator(train_gen, nb_epoch=1,
                                            nb_val_samples = 20000,
                                            samples_per_epoch=16000, #Should be based on the comeplte training data
                                            callbacks=[checkpointer],
                                            validation_data=val_gen)
    return past_history

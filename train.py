from keras.callbacks import ModelCheckpoint
from keras.callbacks import EarlyStopping
from keras.preprocessing.image import ImageDataGenerator


def train(model, data_augmentation, X_train, X_test, Words_train, Words_test, y_train, y_test):
    batch_size = 32
    nb_epoch = 1
    checkpointer = ModelCheckpoint( filepath="weights.hdf5", verbose=1, monitor='val_loss',
                                    save_best_only=True )

    early_stopping = EarlyStopping( monitor='val_loss', patience=10, verbose=0, mode='auto' )

    if not data_augmentation:
        print('Not using data augmentation.')
        past_history = model.fit( [X_train, Words_train], y_train,
                                  batch_size=batch_size,
                                  nb_epoch=1,
                                  validation_data=([X_test, Words_test], y_test),
                                  shuffle=True,
                                  callbacks=[checkpointer, early_stopping] )
    else:
        print('Using real-time data augmentation.')

        # this will do pre-processing and realtime data augmentation
        datagen = ImageDataGenerator( rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)
                                      height_shift_range=0.10,
                                      # randomly shift images horizontally (fraction of total width)
                                      width_shift_range=0.10,
                                      # randomly shift images vertically (fraction of total height)
                                      vertical_flip=False,
                                      zoom_range=0.1,
                                      shear_range=0.1,
                                      zca_whitening=False,
                                      fill_mode="nearest" )  # randomly flip images

        # compute quantities required for featurewise normalization
        # (std, mean, and principal components if ZCA whitening is applied)
        datagen.fit( X_train )
        # fit the model on the batches generated by datagen.flow()
        past_history = model.fit_generator( datagen.flow( [X_train, Words_train], y_train,
                                                          batch_size=batch_size ),
                                            samples_per_epoch=X_train.shape[0],
                                            nb_epoch=nb_epoch,
                                            validation_data=([X_test, Words_test], y_test),
                                            callbacks=[checkpointer, early_stopping] )
    return past_history
